{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ddbb8f-4e35-4d04-bb5d-0d82727862fd",
   "metadata": {},
   "source": [
    "[WIP] GPT2 implementation based on [Neel Nanda's Clean Transformer Video Tutorial](https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2&ab_channel=NeelNanda) and Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29bce165-268a-4c14-a84f-aeaea808a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import unittest\n",
    "from fancy_einsum import einsum\n",
    "import math\n",
    "from easy_transformer import EasyTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05fbcf2c-58d0-4e9f-a671-d559ce64b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db16fb5f-a994-461b-95de-c56ef57cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    max_context: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64a342a1-d3e5-47df-8655-d08422c44f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(residual, 'b p d -> b p', 'mean')\n",
    "        broadcast_mean = einops.repeat(mean,'b p -> b p d', d=cfg.d_model)\n",
    "        residual -= broadcast_mean\n",
    "        std_dev = torch.sqrt(einops.reduce(residual ** 2, 'b p d -> b p', 'mean') + cfg.layer_norm_eps)\n",
    "        broadcast_std_dev = einops.repeat(std_dev, 'b p -> b p d', d=cfg.d_model)\n",
    "        normalized = residual / broadcast_std_dev\n",
    "        return normalized * self.w + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4f73161-7e60-41b2-bf7f-233dad4b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        one_hot_tokens = nn.functional.one_hot(tokens, num_classes = cfg.d_vocab).float()\n",
    "        return torch.matmul(one_hot_tokens, self.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05c31633-e60b-44c6-a5dd-19b253c07b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.max_context, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        batch_size, max_tokens = tokens.shape\n",
    "        truncuated_W_pos = self.W_pos[:max_tokens, :]\n",
    "        return torch.broadcast_to(truncuated_W_pos, (batch_size, max_tokens, self.cfg.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a30a123d-8a54-430b-8c57-ea780e49424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "        else:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cpu\"))\n",
    "            \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        \n",
    "        queries = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "\n",
    "        keys = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_K) + self.b_K\n",
    "\n",
    "        values = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_V) + self.b_V\n",
    "\n",
    "        attention_scores = einsum(\n",
    "            'batch query_position n_heads d_head, batch key_position n_heads d_head -> batch n_heads query_position key_position',\n",
    "            queries,\n",
    "            keys)\n",
    "        attention_scores = attention_scores / math.sqrt(cfg.d_head)\n",
    "\n",
    "        mask = torch.triu(torch.ones(attention_scores.shape[-2], attention_scores.shape[-1]), diagonal=1).bool()\n",
    "        attention_scores.masked_fill_(mask, self.IGNORE)\n",
    "        prob_dist = torch.softmax(attention_scores, dim=3)\n",
    "\n",
    "        sum_after_attention = einsum(\n",
    "            'batch key_position n_heads d_head, batch n_heads query_position key_position -> batch n_heads query_position d_head',\n",
    "            values,\n",
    "            prob_dist)\n",
    "        print(sum_after_attention.sum())\n",
    "        out_per_heads = einsum(\n",
    "            'batch n_heads query_position d_head, n_heads d_head d_model -> batch n_heads query_position d_model',\n",
    "            sum_after_attention,\n",
    "            self.W_O) + self.b_O\n",
    "        print('aaaaaaaaaa')\n",
    "        print(out_per_heads.sum( dim=1).sum())\n",
    "        print(out_per_heads.sum())\n",
    "\n",
    "        out_per_heads = einsum(\n",
    "            'batch n_heads query_position d_head, n_heads d_head d_model -> batch query_position d_model',\n",
    "            sum_after_attention,\n",
    "            self.W_O) + self.b_O\n",
    "        print('bbbbb')\n",
    "        print(out_per_heads.sum())\n",
    "        \n",
    "        return out_per_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f1c654f9-35ba-43b3-9149-22689a90c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "3b27893b-f9a2-42e1-8628-ee41a364dae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = torch.rand(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6c5a67cc-b1cc-4e18-92ed-53dbe8a7a304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1533, 0.0137, 0.7793],\n",
       "        [0.3396, 0.0423, 0.4133]])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "einsum('i j, i j -> i j', a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f1952b56-6623-4fc6-8dca-bf09ecdfb9a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8646, 0.1245, 0.8098],\n",
       "        [0.5283, 0.5886, 0.4926]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f244e94b-8fe9-4b83-b9f4-3041bf28729d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1774, 0.1100, 0.9624],\n",
       "        [0.6429, 0.0719, 0.8390]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308164f4-58f4-4273-869d-660f656d8d95",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b593f57-3294-47e4-bc7a-fcfc921937da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c268d88f-a757-4c80-896e-67274f17aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.reference_gpt2 = self.get_reference_gpt2()\n",
    "        reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "        self.tokens = self.reference_gpt2.to_tokens(reference_text)\n",
    "        if IN_COLAB:\n",
    "            self.tokens = self.tokens.cuda()\n",
    "        self.cache_dict = self.get_gpt2_cache_dict(self.tokens)\n",
    "        self.cfg = Config(debug=True)\n",
    "        \n",
    "    def get_reference_gpt2(self):\n",
    "        return EasyTransformer.from_pretrained(\n",
    "            \"gpt2-small\",\n",
    "            fold_ln=False,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False)\n",
    "\n",
    "    def get_gpt2_cache_dict(self, tokens):    \n",
    "        _, cache = self.reference_gpt2.run_with_cache(tokens)\n",
    "        return cache.cache_dict\n",
    "\n",
    "    def rand_float_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randn(shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def rand_int_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randint(100, 1000, shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def load_gpt2_test(self, cls, gpt2_layer, input_name):\n",
    "        layer = cls(cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "        # Allow inputs of strings or tensors\n",
    "        if isinstance(input_name, str):\n",
    "            reference_input = self.cache_dict[input_name]\n",
    "        else:\n",
    "            reference_input = input_name\n",
    "        output = layer(reference_input)\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "        comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "        correct_ratio = comparison.sum()/comparison.numel()\n",
    "        self.assertEqual(correct_ratio, 1, f'{torch.round(correct_ratio * 100)}% of values are correct')\n",
    "        return output\n",
    "\n",
    "    def test_layer_norm(self):\n",
    "        self.rand_float_test(LayerNorm, [2, 4, 768])\n",
    "        self.load_gpt2_test(LayerNorm, self.reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")\n",
    "\n",
    "    def test_embed(self):\n",
    "        self.rand_int_test(Embed, [2, 4])\n",
    "        self.load_gpt2_test(Embed, self.reference_gpt2.embed, self.tokens)\n",
    "\n",
    "    def test_pos_embed(self):\n",
    "        self.rand_int_test(PosEmbed, [2, 4])\n",
    "        self.load_gpt2_test(PosEmbed, self.reference_gpt2.pos_embed, self.tokens)\n",
    "\n",
    "    def test_attention(self):\n",
    "        # self.rand_float_test(Attention, [2, 4, 768])\n",
    "        self.load_gpt2_test(Attention, self.reference_gpt2.blocks[0].attn, \"blocks.0.ln1.hook_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "19fed372-db51-4d61-8fcd-c547ab5b1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 3.058s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(157.6061, grad_fn=<SumBackward0>)\n",
      "aaaaaaaaaa\n",
      "tensor(-1716.8521, grad_fn=<SumBackward0>)\n",
      "tensor(-1716.8517, grad_fn=<SumBackward0>)\n",
      "bbbbb\n",
      "tensor(326.3628, grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestSuite()\n",
    "# suite.addTest(Tests('test_layer_norm'))\n",
    "# suite.addTest(Tests('test_embed'))\n",
    "# suite.addTest(Tests('test_pos_embed'))\n",
    "suite.addTest(Tests('test_attention'))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d5531d85-347d-47fb-bb0b-3aa46869e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "        else:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cpu\"))\n",
    "            \n",
    "    \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        if self.cfg.debug: print(\"Normalized_resid_pre:\", normalized_resid_pre.shape)\n",
    "        q = einsum(\"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\", normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "        k = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_K) + self.b_K\n",
    "        \n",
    "        attn_scores = einsum(\"batch query_pos n_heads d_head, batch key_pos n_heads d_head -> batch n_heads query_pos key_pos\", q, k)\n",
    "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
    "\n",
    "        attn_scores = self.apply_causal_mask(attn_scores)\n",
    "\n",
    "        pattern = attn_scores.softmax(dim=-1) # [batch, n_head, query_pos, key_pos]\n",
    "\n",
    "        \n",
    "        v = einsum(\"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\", normalized_resid_pre, self.W_V) + self.b_V\n",
    "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", pattern, v)\n",
    "        print(z.sum())\n",
    "        attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_O) + self.b_O\n",
    "        print(attn_out)\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "550d12a2-4bca-456b-ad72-7bd1290e744e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 3.229s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "tensor(157.6061, grad_fn=<SumBackward0>)\n",
      "tensor([[[ 7.9663e-01,  1.6985e-02,  3.4781e-02,  ...,  3.3120e-02,\n",
      "          -2.3129e-02,  1.8103e-01],\n",
      "         [ 1.3167e-03,  1.5750e-01, -1.4059e-01,  ..., -8.1997e-03,\n",
      "           5.3075e-03,  1.3511e-01],\n",
      "         [ 8.9738e-02, -7.2411e-01, -6.9866e-01,  ...,  5.5321e-02,\n",
      "           2.7958e-03,  9.0785e-02],\n",
      "         ...,\n",
      "         [-3.0286e-01,  4.9638e-02, -6.0990e-01,  ..., -3.7084e-02,\n",
      "          -4.9524e-04, -8.6008e-03],\n",
      "         [-1.0844e+00, -6.1457e-02,  2.2966e-01,  ..., -2.6688e-02,\n",
      "          -1.4368e-02,  3.3245e-02],\n",
      "         [ 3.7947e-01, -4.9886e-01,  2.6434e-01,  ..., -2.7894e-02,\n",
      "          -8.9028e-03,  4.8796e-02]]], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestSuite()\n",
    "# suite.addTest(Tests('test_layer_norm'))\n",
    "# suite.addTest(Tests('test_embed'))\n",
    "# suite.addTest(Tests('test_pos_embed'))\n",
    "suite.addTest(Tests('test_attention'))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec75f2-0c13-4c73-a37d-a2ca0db3e852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1747201f-0a2d-4f6f-bb1c-604792ebc006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
