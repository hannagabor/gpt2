{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ddbb8f-4e35-4d04-bb5d-0d82727862fd",
   "metadata": {},
   "source": [
    "[WIP] GPT2 implementation based on [Neel Nanda's Clean Transformer Video Tutorial](https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2&ab_channel=NeelNanda) and Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29bce165-268a-4c14-a84f-aeaea808a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import unittest\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fbcf2c-58d0-4e9f-a671-d559ce64b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db16fb5f-a994-461b-95de-c56ef57cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    max_context: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64a342a1-d3e5-47df-8655-d08422c44f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(residual, 'b p d -> b p', 'mean')\n",
    "        broadcast_mean = einops.repeat(mean,'b p -> b p d', d=cfg.d_model)\n",
    "        residual -= broadcast_mean\n",
    "        std_dev = torch.sqrt(einops.reduce(residual ** 2, 'b p d -> b p', 'mean') + cfg.layer_norm_eps)\n",
    "        broadcast_std_dev = einops.repeat(std_dev, 'b p -> b p d', d=cfg.d_model)\n",
    "        normalized = residual / broadcast_std_dev\n",
    "        return normalized * self.w + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f73161-7e60-41b2-bf7f-233dad4b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        one_hot_tokens = nn.functional.one_hot(tokens, num_classes = cfg.d_vocab).float()\n",
    "        return torch.matmul(one_hot_tokens, self.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05c31633-e60b-44c6-a5dd-19b253c07b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.max_context, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        batch_size, max_tokens = tokens.shape\n",
    "        truncuated_W_pos = self.W_pos[:max_tokens, :]\n",
    "        return torch.broadcast_to(truncuated_W_pos, (batch_size, max_tokens, self.cfg.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30a123d-8a54-430b-8c57-ea780e49424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "    \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        batch_size, max_position, _ = normalized_resid_pre.shape\n",
    "        broadcast_b_Q = torch.broadcast_to(\n",
    "            [batch_size, max_position, cfg.n_heads, cfg.d_head],\n",
    "            self.b_Q)\n",
    "        queries = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_head d_head',\n",
    "            normalized_resid_pre, self.W_Q) + broadcast_b_Q\n",
    "\n",
    "        broadcast_b_K = torch.broadcast_to(\n",
    "            [batch_size, max_position, cfg.n_heads, cfg.d_head],\n",
    "            self.b_K)\n",
    "        keys = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_head d_head',\n",
    "            normalized_resid_pre, self.W_K) + broadcast_b_Q\n",
    "\n",
    "    def apply_causal_mask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]\n",
    "        \"YOUR CODE HERE\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b5c9df-7e26-4043-b211-af9bbffeb552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c268d88f-a757-4c80-896e-67274f17aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.reference_gpt2 = self.get_reference_gpt2()\n",
    "        reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "        self.tokens = self.reference_gpt2.to_tokens(reference_text)\n",
    "        if IN_COLAB:\n",
    "            self.tokens = self.tokens.cuda()\n",
    "        self.cache_dict = self.get_gpt2_cache_dict(self.tokens)\n",
    "        self.cfg = Config(debug=True)\n",
    "        \n",
    "    def get_reference_gpt2(self):\n",
    "        return transformer_lens.HookedTransformer.from_pretrained(\n",
    "            \"gpt2-small\",\n",
    "            fold_ln=False,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False)\n",
    "\n",
    "    def get_gpt2_cache_dict(self, tokens):    \n",
    "        _, cache = self.reference_gpt2.run_with_cache(tokens)\n",
    "        return cache.cache_dict\n",
    "\n",
    "    def rand_float_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randn(shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def rand_int_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randint(100, 1000, shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def load_gpt2_test(self, cls, gpt2_layer, input_name):\n",
    "        layer = cls(cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "        # Allow inputs of strings or tensors\n",
    "        if isinstance(input_name, str):\n",
    "            reference_input = self.cache_dict[input_name]\n",
    "        else:\n",
    "            reference_input = input_name\n",
    "        output = layer(reference_input)\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "        comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "        correct_ratio = comparison.sum()/comparison.numel()\n",
    "        self.assertEqual(correct_ratio, 1)\n",
    "        return output\n",
    "\n",
    "    def test_layer_norm(self):\n",
    "        self.rand_float_test(LayerNorm, [2, 4, 768])\n",
    "        self.load_gpt2_test(LayerNorm, self.reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")\n",
    "\n",
    "    def test_embed(self):\n",
    "        self.rand_int_test(Embed, [2, 4])\n",
    "        self.load_gpt2_test(Embed, self.reference_gpt2.embed, self.tokens)\n",
    "\n",
    "    def test_pos_embed(self):\n",
    "        self.rand_int_test(PosEmbed, [2, 4])\n",
    "        self.load_gpt2_test(PosEmbed, self.reference_gpt2.pos_embed, self.tokens)\n",
    "\n",
    "    def test_attention(self):\n",
    "        self.rand_float_test(Attention, [2, 4, 768])\n",
    "        self.load_gpt2_test(Attention, self.reference_gpt2.blocks[0].attn, \"blocks.0.ln1.hook_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19fed372-db51-4d61-8fcd-c547ab5b1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".Using pad_token, but it is not set yet.\n",
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 9.144s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestSuite()\n",
    "suite.addTest(Tests('test_layer_norm'))\n",
    "suite.addTest(Tests('test_embed'))\n",
    "suite.addTest(Tests('test_pos_embed'))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ccd14-517d-4e7e-add8-08249c0f8599",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
