{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ddbb8f-4e35-4d04-bb5d-0d82727862fd",
   "metadata": {},
   "source": [
    "[WIP] GPT2 implementation based on [Neel Nanda's Clean Transformer Video Tutorial](https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2&ab_channel=NeelNanda) and Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29bce165-268a-4c14-a84f-aeaea808a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import unittest\n",
    "from fancy_einsum import einsum\n",
    "import math\n",
    "from easy_transformer import EasyTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05fbcf2c-58d0-4e9f-a671-d559ce64b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db16fb5f-a994-461b-95de-c56ef57cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    max_context: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64a342a1-d3e5-47df-8655-d08422c44f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(residual, 'b p d -> b p', 'mean')\n",
    "        broadcast_mean = einops.repeat(mean,'b p -> b p d', d=cfg.d_model)\n",
    "        residual -= broadcast_mean\n",
    "        std_dev = torch.sqrt(einops.reduce(residual ** 2, 'b p d -> b p', 'mean') + cfg.layer_norm_eps)\n",
    "        broadcast_std_dev = einops.repeat(std_dev, 'b p -> b p d', d=cfg.d_model)\n",
    "        normalized = residual / broadcast_std_dev\n",
    "        return normalized * self.w + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4f73161-7e60-41b2-bf7f-233dad4b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        one_hot_tokens = nn.functional.one_hot(tokens, num_classes = cfg.d_vocab).float()\n",
    "        return torch.matmul(one_hot_tokens, self.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05c31633-e60b-44c6-a5dd-19b253c07b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.max_context, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        batch_size, max_tokens = tokens.shape\n",
    "        truncuated_W_pos = self.W_pos[:max_tokens, :]\n",
    "        return torch.broadcast_to(truncuated_W_pos, (batch_size, max_tokens, self.cfg.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a30a123d-8a54-430b-8c57-ea780e49424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_query = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_query, std=self.cfg.init_range)\n",
    "        self.b_query = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_key = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_key, std=self.cfg.init_range)\n",
    "        self.b_key = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_value = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_value, std=self.cfg.init_range)\n",
    "        self.b_value = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        \n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "\n",
    "        if IN_COLAB:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "        else:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cpu\"))\n",
    "            \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "\n",
    "        queries = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_query) + self.b_query\n",
    "\n",
    "        keys = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_key) + self.b_key\n",
    "\n",
    "        values = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_value) + self.b_value\n",
    "        \n",
    "        attention_scores = einsum(\n",
    "            'batch query_position n_heads d_head, batch key_position n_heads d_head -> batch n_heads query_position key_position',\n",
    "            queries,\n",
    "            keys)\n",
    "        attention_scores = attention_scores / math.sqrt(cfg.d_head)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(attention_scores.shape[-2], attention_scores.shape[-1]), diagonal=1).bool()\n",
    "        attention_scores.masked_fill_(mask, self.IGNORE)\n",
    "        prob_dist = torch.softmax(attention_scores, dim=3)\n",
    "\n",
    "        z = einsum(\"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", prob_dist, values)\n",
    "\n",
    "        # attn_out = einsum(\"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", z, self.W_out) + self.b_out\n",
    "        # print(attn_out)\n",
    "        # return attn_out\n",
    "\n",
    "        \n",
    "        sum_after_attention = einsum(\n",
    "            'batch key_position n_heads d_head, batch n_heads query_position key_position -> batch n_heads query_position d_head',\n",
    "            values,\n",
    "            prob_dist)\n",
    "\n",
    "        out_per_heads = einsum(\n",
    "            'batch n_heads query_position d_head, n_heads d_head d_model -> batch n_heads query_position d_model',\n",
    "            sum_after_attention,\n",
    "            self.W_out) + self.b_out\n",
    "\n",
    "        print(normalized_resid_pre + torch.sum(out_per_heads, dim=1))\n",
    "\n",
    "        return normalized_resid_pre + torch.sum(out_per_heads, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308164f4-58f4-4273-869d-660f656d8d95",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c268d88f-a757-4c80-896e-67274f17aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tests(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.reference_gpt2 = self.get_reference_gpt2()\n",
    "        reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "        self.tokens = self.reference_gpt2.to_tokens(reference_text)\n",
    "        if IN_COLAB:\n",
    "            self.tokens = self.tokens.cuda()\n",
    "        self.cache_dict = self.get_gpt2_cache_dict(self.tokens)\n",
    "        self.cfg = Config(debug=True)\n",
    "        \n",
    "    def get_reference_gpt2(self):\n",
    "        return EasyTransformer.from_pretrained(\n",
    "            \"gpt2-small\",\n",
    "            fold_ln=False,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False)\n",
    "\n",
    "    def get_gpt2_cache_dict(self, tokens):    \n",
    "        _, cache = self.reference_gpt2.run_with_cache(tokens)\n",
    "        return cache.cache_dict\n",
    "\n",
    "    def rand_float_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randn(shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def rand_int_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randint(100, 1000, shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def load_gpt2_test(self, cls, gpt2_layer, input_name):\n",
    "        layer = cls(cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "        # Allow inputs of strings or tensors\n",
    "        if isinstance(input_name, str):\n",
    "            reference_input = self.cache_dict[input_name]\n",
    "        else:\n",
    "            reference_input = input_name\n",
    "        print(reference_input.shape)\n",
    "        output = layer(reference_input)\n",
    "        print(output.shape)\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "        print(reference_output.shape)\n",
    "        comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "        correct_ratio = comparison.sum()/comparison.numel()\n",
    "        self.assertEqual(correct_ratio, 1, f'{torch.round(correct_ratio * 100)}% of values are correct')\n",
    "        return output\n",
    "\n",
    "    def test_layer_norm(self):\n",
    "        self.rand_float_test(LayerNorm, [2, 4, 768])\n",
    "        self.load_gpt2_test(LayerNorm, self.reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")\n",
    "\n",
    "    def test_embed(self):\n",
    "        self.rand_int_test(Embed, [2, 4])\n",
    "        self.load_gpt2_test(Embed, self.reference_gpt2.embed, self.tokens)\n",
    "\n",
    "    def test_pos_embed(self):\n",
    "        self.rand_int_test(PosEmbed, [2, 4])\n",
    "        self.load_gpt2_test(PosEmbed, self.reference_gpt2.pos_embed, self.tokens)\n",
    "\n",
    "    def test_attention(self):\n",
    "        self.rand_float_test(Attention, [2, 4, 768])\n",
    "        self.load_gpt2_test(Attention, self.reference_gpt2.blocks[0].attn, \"blocks.0.ln1.hook_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19fed372-db51-4d61-8fcd-c547ab5b1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F\n",
      "======================================================================\n",
      "FAIL: test_attention (__main__.Tests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_8680/1241486984.py\", line 76, in test_attention\n",
      "    self.load_gpt2_test(Attention, self.reference_gpt2.blocks[0].attn, \"blocks.0.ln1.hook_normalized\")\n",
      "  File \"/tmp/ipykernel_8680/1241486984.py\", line 59, in load_gpt2_test\n",
      "    self.assertEqual(correct_ratio, 1, f'{torch.round(correct_ratio * 100)}% of values are correct')\n",
      "AssertionError: tensor(0.0007) != 1 : 0.0% of values are correct\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 1 test in 3.956s\n",
      "\n",
      "FAILED (failures=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.4781e+00,  9.4271e-02,  1.1739e+00,  ...,  4.5701e-01,\n",
      "          -5.3349e-01, -1.5206e+00],\n",
      "         [ 9.9553e-01,  1.1775e+00, -2.8085e-01,  ...,  1.1971e+00,\n",
      "          -1.9991e-01, -1.8756e+00],\n",
      "         [ 8.9338e-02,  1.2168e-04, -1.7418e+00,  ...,  1.2769e+00,\n",
      "          -4.6312e-01, -1.0442e+00],\n",
      "         [ 1.5478e+00, -1.9147e+00, -7.7552e-01,  ...,  1.0997e+00,\n",
      "          -7.1781e-01, -1.0318e+00]],\n",
      "\n",
      "        [[ 7.6765e-01,  1.6938e-01,  7.3310e-02,  ..., -1.4813e+00,\n",
      "           2.3008e-01, -2.2283e-01],\n",
      "         [ 2.3846e+00,  1.6257e+00,  5.1552e-01,  ...,  1.0848e+00,\n",
      "          -8.0254e-01, -1.6505e-01],\n",
      "         [ 6.4801e-01,  9.7900e-02,  2.8726e-01,  ..., -5.5491e-01,\n",
      "          -1.6021e+00, -3.6980e-01],\n",
      "         [-1.4114e+00,  3.2381e+00,  1.8587e-01,  ..., -2.7156e-01,\n",
      "          -7.7272e-01,  4.2269e-01]]], grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 35, 768])\n",
      "tensor([[[ 0.0184, -0.0613, -0.0540,  ..., -0.0196,  0.1006,  0.0794],\n",
      "         [ 0.1735, -0.0779, -0.0592,  ...,  0.1194, -0.0393, -0.1159],\n",
      "         [ 0.1818, -0.1409,  0.0288,  ...,  0.2565,  0.0264, -0.0250],\n",
      "         ...,\n",
      "         [-0.0530,  0.0630,  0.0104,  ..., -0.0502,  0.0459, -0.0854],\n",
      "         [-0.1984,  0.2196, -0.0267,  ..., -0.3429,  0.1928,  0.0180],\n",
      "         [-0.1369,  0.0101, -0.0075,  ..., -0.1474, -0.0162,  0.0218]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "torch.Size([1, 35, 768])\n",
      "torch.Size([1, 35, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=1 errors=0 failures=1>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestSuite()\n",
    "# suite.addTest(Tests('test_layer_norm'))\n",
    "# suite.addTest(Tests('test_embed'))\n",
    "# suite.addTest(Tests('test_pos_embed'))\n",
    "suite.addTest(Tests('test_attention'))\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5531d85-347d-47fb-bb0b-3aa46869e3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec75f2-0c13-4c73-a37d-a2ca0db3e852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
