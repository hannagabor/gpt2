{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ddbb8f-4e35-4d04-bb5d-0d82727862fd",
   "metadata": {},
   "source": [
    "[WIP] GPT2 implementation based on [Neel Nanda's Clean Transformer Video Tutorial](https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2&ab_channel=NeelNanda) and Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bce165-268a-4c14-a84f-aeaea808a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import unittest\n",
    "from fancy_einsum import einsum\n",
    "import math\n",
    "from easy_transformer import EasyTransformer\n",
    "from easy_transformer.utils import gelu_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db16fb5f-a994-461b-95de-c56ef57cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    max_context: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a342a1-d3e5-47df-8655-d08422c44f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(residual, 'batch position d_model -> batch position', 'mean')\n",
    "        broadcast_mean = einops.repeat(mean,'batch position -> batch position d_model', d_model=self.cfg.d_model)\n",
    "        residual = residual - broadcast_mean\n",
    "        # TODO: For some reason, residual -= broadcast_mean makes the TransformerBlock test fail. Why?\n",
    "        std_dev = torch.sqrt(einops.reduce(residual ** 2, 'batch position d_model -> batch position', 'mean') + self.cfg.layer_norm_eps)\n",
    "        broadcast_std_dev = einops.repeat(std_dev, 'batch position -> batch position d_model', d_model=self.cfg.d_model)\n",
    "        normalized = residual / broadcast_std_dev\n",
    "        return normalized * self.w + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f73161-7e60-41b2-bf7f-233dad4b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        one_hot_tokens = nn.functional.one_hot(tokens, num_classes = self.cfg.d_vocab).float()\n",
    "        return one_hot_tokens @ self.W_E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c31633-e60b-44c6-a5dd-19b253c07b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.max_context, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        batch_size, max_tokens = tokens.shape\n",
    "        truncuated_W_pos = self.W_pos[:max_tokens, :]\n",
    "        return torch.broadcast_to(truncuated_W_pos, (batch_size, max_tokens, self.cfg.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a30a123d-8a54-430b-8c57-ea780e49424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Parameters to calculate queries\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        # Parameters to calculate keys\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        # Parameters to calculate values\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        # Parameters to combine head outputs\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "            \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        \n",
    "        queries = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "\n",
    "        keys = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_K) + self.b_K\n",
    "\n",
    "        values = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_V) + self.b_V\n",
    "\n",
    "        prob_dist = self._get_attention(queries, keys)\n",
    "\n",
    "        sum_after_attention = einsum(\n",
    "            'batch key_position n_heads d_head, batch n_heads query_position key_position -> batch n_heads query_position d_head',\n",
    "            values,\n",
    "            prob_dist)\n",
    "        \n",
    "        out = einsum(\n",
    "            'batch n_heads query_position d_head, n_heads d_head d_model -> batch query_position d_model',\n",
    "            sum_after_attention,\n",
    "            self.W_O) + self.b_O\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def _get_attention(self, queries, keys):\n",
    "        attention_scores = einsum(\n",
    "            'batch query_position n_heads d_head, batch key_position n_heads d_head -> batch n_heads query_position key_position',\n",
    "            queries,\n",
    "            keys)\n",
    "        attention_scores = attention_scores / math.sqrt(self.cfg.d_head)\n",
    "        mask = torch.triu(torch.ones(attention_scores.shape[-2], attention_scores.shape[-1]), diagonal=1).bool().cuda()\n",
    "        attention_scores.masked_fill_(mask, self.IGNORE)\n",
    "        prob_dist = torch.softmax(attention_scores, dim=3)\n",
    "        return prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606efaac-a712-404a-b39a-55d091a10872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "    \n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        middle = einsum(\n",
    "            'd_model d_mlp, batch position d_model-> batch position d_mlp',\n",
    "            self.W_in,\n",
    "            normalized_resid_mid) + self.b_in\n",
    "        after_non_lin = gelu_new(middle)\n",
    "        out = einsum(\n",
    "            'd_mlp d_model, batch position d_mlp -> batch position d_model',\n",
    "            self.W_out,\n",
    "            after_non_lin) + self.b_out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b93cf7-6cc6-4887-9351-cda927fad60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        attn = self.attn(self.ln1(resid_pre))\n",
    "        resid_mid = resid_pre + attn\n",
    "        mlp_out = self.mlp(self.ln2(resid_mid))\n",
    "        out = resid_mid + mlp_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e49d556f-ec53-4281-8c33-472cd06a783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.W_U = nn.Parameter(torch.empty((cfg.d_model, cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((cfg.d_vocab), requires_grad=False))\n",
    "    \n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final [batch, position, d_model]\n",
    "        return einsum(\n",
    "            'd_model d_vocab, batch position d_model -> batch position d_vocab',\n",
    "            self.W_U,\n",
    "            normalized_resid_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e487fd2-2453-4b76-ba17-e1dc60f91a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens [batch, position]\n",
    "        res = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            res = block(res)\n",
    "        normalized_res = self.ln_final(res)\n",
    "        logits = self.unembed(normalized_res)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308164f4-58f4-4273-869d-660f656d8d95",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c268d88f-a757-4c80-896e-67274f17aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tests(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.reference_gpt2 = cls.get_reference_gpt2()\n",
    "        reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "        cls.tokens = cls.reference_gpt2.to_tokens(reference_text)\n",
    "        cls.tokens = cls.tokens.cuda()\n",
    "        cls.cache = cls.get_gpt2_cache_dict(cls.tokens)\n",
    "        cls.cfg = Config(debug=True)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_reference_gpt2(cls):\n",
    "        return EasyTransformer.from_pretrained(\n",
    "            \"gpt2-small\",\n",
    "            fold_ln=False,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False)\n",
    "\n",
    "    @classmethod\n",
    "    def get_gpt2_cache_dict(cls, tokens):    \n",
    "        _, cache = cls.reference_gpt2.run_with_cache(tokens)\n",
    "        return cache\n",
    "\n",
    "    def rand_float_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        layer = layer.cuda()\n",
    "        random_input = torch.randn(shape)\n",
    "        random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def rand_int_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        layer = layer.cuda()\n",
    "        random_input = torch.randint(100, 1000, shape)\n",
    "        random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def load_gpt2_test(self, cls, gpt2_layer, input_name):\n",
    "        layer = cls(self.cfg)\n",
    "        layer = layer.cuda()\n",
    "        layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "        # Allow inputs of strings or tensors\n",
    "        if isinstance(input_name, str):\n",
    "            reference_input = self.cache.cache_dict[input_name]\n",
    "        else:\n",
    "            reference_input = input_name\n",
    "        reference_input = reference_input.cuda()\n",
    "        output = layer(reference_input)\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "        comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "        correct_ratio = comparison.sum()/comparison.numel()\n",
    "        self.assertEqual(correct_ratio, 1, f'{torch.round(correct_ratio * 100)}% of values are correct')\n",
    "        return output\n",
    "\n",
    "    def test_layer_norm(self):\n",
    "        self.rand_float_test(LayerNorm, [2, 4, 768])\n",
    "        self.load_gpt2_test(LayerNorm, self.reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")\n",
    "\n",
    "    def test_embed(self):\n",
    "        self.rand_int_test(Embed, [2, 4])\n",
    "        self.load_gpt2_test(Embed, self.reference_gpt2.embed, self.tokens)\n",
    "\n",
    "    def test_pos_embed(self):\n",
    "        self.rand_int_test(PosEmbed, [2, 4])\n",
    "        self.load_gpt2_test(PosEmbed, self.reference_gpt2.pos_embed, self.tokens)\n",
    "\n",
    "    def test_attention(self):\n",
    "        self.rand_float_test(Attention, [2, 4, 768])\n",
    "        self.load_gpt2_test(Attention, self.reference_gpt2.blocks[0].attn, \"blocks.0.ln1.hook_normalized\")\n",
    "\n",
    "    def test_mlp(self):\n",
    "        self.rand_float_test(MLP, [2, 4, 768])\n",
    "        self.load_gpt2_test(MLP, self.reference_gpt2.blocks[0].mlp, \"blocks.0.ln2.hook_normalized\")\n",
    "\n",
    "    def test_transformer_block(self):\n",
    "        self.rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "        self.load_gpt2_test(TransformerBlock, self.reference_gpt2.blocks[0], self.cache[\"resid_pre\", 0])\n",
    "\n",
    "    def test_unembed(self):\n",
    "        self.rand_float_test(Unembed, [2, 4, 768])\n",
    "        self.load_gpt2_test(Unembed, self.reference_gpt2.unembed, \"ln_final.hook_normalized\")\n",
    "\n",
    "    def test_full_transformer(self):\n",
    "        self.rand_int_test(DemoTransformer, [2, 4])\n",
    "        self.load_gpt2_test(DemoTransformer, self.reference_gpt2, self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19fed372-db51-4d61-8fcd-c547ab5b1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "........\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 7.754s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=8 errors=0 failures=0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestSuite()\n",
    "suite.addTest(Tests('test_layer_norm'))\n",
    "suite.addTest(Tests('test_embed'))\n",
    "suite.addTest(Tests('test_pos_embed'))\n",
    "suite.addTest(Tests('test_attention'))\n",
    "suite.addTest(Tests('test_mlp'))\n",
    "suite.addTest(Tests('test_transformer_block'))\n",
    "suite.addTest(Tests('test_unembed'))\n",
    "suite.addTest(Tests('test_full_transformer'))\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a095648a-c330-4c65-bfbe-5db1040d66b4",
   "metadata": {},
   "source": [
    "## Try with pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "019db6c0-92c4-49f5-a181-aba69224a524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cuda\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=False))\n",
    "reference_gpt2 = EasyTransformer.from_pretrained(\n",
    "            \"gpt2-small\",\n",
    "            fold_ln=False,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False)\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "demo_gpt2.cuda()\n",
    "\n",
    "test_string = \"\"\"Mini scule is a species of microhylid frog endemic to Madagascar that was described in 2019. The scientific name of the species refers to its size, being a pun on the word minuscule. It is very small, measuring only 8.4 to 10.8 mm (0.33 to 0.43 in) in snout–vent length. It has bronze underparts with a brown groin and back of the thigh, cream upperparts with brown flecking, a dark brown side of the head, and a red iris. On the hind feet, the first toe is absent and the second and fifth toes are strongly reduced. The frog is known only from the Sainte Luce Reserve, where it inhabits areas with deep leaf litter near semi-permanent water bodies. Specimens of frogs from Mandena, the Vohimena mountains, the southern Anosy Mountains, and Tsitongambarika may also be of this species. Along with Mini mum and Mini ature, the other two species in its genus, it received media attention when first described due to the wordplay in its scientific name. (Full article...)\"\"\"\n",
    "test_tokens = reference_gpt2.to_tokens(test_string).cuda()\n",
    "demo_logits = demo_gpt2(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7bc91a6-0790-4c69-a658-0baee8adeb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Loss as average prob tensor(0.0243, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Loss as 'uniform over this many variables' tensor(41.2080, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Uniform loss over the vocab 10.82490511970208\n"
     ]
    }
   ],
   "source": [
    "# logits is [batch, position, vocab_size]\n",
    "b, p, v = demo_logits.shape\n",
    "reshaped_logits = demo_logits[:, :-1,:].view(b * (p - 1), v)\n",
    "reshaped_targets = test_tokens[:,1:].view(b * (p - 1))\n",
    "loss = torch.nn.functional.cross_entropy(reshaped_logits, reshaped_targets)\n",
    "print(loss)\n",
    "print(\"Loss as average prob\", (-loss).exp())\n",
    "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
    "print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c0ef87-fa93-478c-8ab7-79e8018fa653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
