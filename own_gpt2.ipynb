{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ddbb8f-4e35-4d04-bb5d-0d82727862fd",
   "metadata": {},
   "source": [
    "GPT2 implementation [WIP] based on [Neel Nanda's Clean Transformer Template](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/clean-transformer-demo/Clean_Transformer_Demo_Template.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bce165-268a-4c14-a84f-aeaea808a887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanna/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from easy_transformer import EasyTransformer\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fbcf2c-58d0-4e9f-a671-d559ce64b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561893cf-ff7d-4dd9-a537-919297d620f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    }
   ],
   "source": [
    "reference_gpt2 = EasyTransformer.from_pretrained(\"gpt2-small\", fold_ln=False, center_unembed=False, center_writing_weights=False)\n",
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "if IN_COLAB:\n",
    "    tokens = tokens.cuda()\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db16fb5f-a994-461b-95de-c56ef57cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f28e4dd8-ccff-4aea-8508-7d0fc4d60c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    if IN_COLAB:\n",
    "        layer = layer.cuda()\n",
    "    random_input = torch.randn(shape)\n",
    "    if IN_COLAB:\n",
    "        random_input = random_input.cuda()\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    if IN_COLAB:\n",
    "        layer = layer.cuda()\n",
    "    random_input = torch.randint(100, 1000, shape)\n",
    "    if IN_COLAB:\n",
    "        random_input = random_input.cuda()\n",
    "    print(\"Input shape:\", random_input.shape)\n",
    "    output = layer(random_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print()\n",
    "    return output\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg)\n",
    "    if IN_COLAB:\n",
    "        layer = layer.cuda()\n",
    "    layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "    # Allow inputs of strings or tensors\n",
    "    if isinstance(input_name, str):\n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(\"Input shape:\", reference_input.shape)\n",
    "    output = layer(reference_input)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    reference_output = gpt2_layer(reference_input)\n",
    "    print(\"Reference output shape:\", reference_output.shape)\n",
    "    comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a342a1-d3e5-47df-8655-d08422c44f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(residual, 'b p d -> b p', 'mean')\n",
    "        broadcast_mean = einops.repeat(mean,'b p -> b p d', d=cfg.d_model)\n",
    "        print(residual.shape)\n",
    "        print(broadcast_mean.shape)\n",
    "        residual -= broadcast_mean\n",
    "        std_dev = torch.sqrt(einops.reduce(residual ** 2, 'b p d -> b p', 'mean') + cfg.layer_norm_eps)\n",
    "        broadcast_std_dev = einops.repeat(std_dev, 'b p -> b p d', d=cfg.d_model)\n",
    "        normalized = residual / broadcast_std_dev\n",
    "        return normalized * self.w + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15b1960a-0a09-404a-8875-fe64c7de3cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35, 768])\n",
      "torch.Size([1, 35, 768])\n",
      "torch.Size([1, 35, 768])\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0667,  0.0881, -0.3085,  ...,  0.0307,  0.0512, -0.0019],\n",
       "         [ 0.0278, -0.2843,  0.2504,  ...,  0.0993,  0.0567,  0.0519],\n",
       "         [-0.5468, -0.5119, -0.6429,  ...,  0.2615, -0.1498,  0.1759],\n",
       "         ...,\n",
       "         [ 0.3988, -0.1717,  0.0907,  ..., -0.0095,  0.5077,  0.1327],\n",
       "         [-0.0164, -0.3170, -1.5848,  ..., -0.0970,  0.3219,  0.1132],\n",
       "         [ 0.2771, -0.4338, -0.2735,  ...,  0.1036, -0.1892,  0.0365]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_float_test(LayerNorm, [2, 4, 768])\n",
    "load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f73161-7e60-41b2-bf7f-233dad4b972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "torch.Size([2, 4, 50257])\n",
      "torch.Size([50257, 768])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Parameter containing:\n",
      "tensor([[-0.0062,  0.0263,  0.0154,  ..., -0.0125, -0.0275, -0.0333],\n",
      "        [-0.0087,  0.0454,  0.0400,  ..., -0.0160,  0.0122, -0.0161],\n",
      "        [-0.0011, -0.0049,  0.0183,  ...,  0.0088,  0.0225,  0.0035],\n",
      "        ...,\n",
      "        [ 0.0214, -0.0059, -0.0252,  ..., -0.0079,  0.0016, -0.0199],\n",
      "        [ 0.0416, -0.0020,  0.0497,  ..., -0.0060, -0.0022,  0.0104],\n",
      "        [ 0.0101,  0.0414, -0.0059,  ...,  0.0102, -0.0337, -0.0387]],\n",
      "       requires_grad=True)\n",
      "Output shape: torch.Size([2, 4, 768])\n",
      "\n",
      "Input shape: torch.Size([1, 35])\n",
      "torch.Size([1, 35, 50257])\n",
      "torch.Size([50257, 768])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [1., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Parameter containing:\n",
      "tensor([[-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453],\n",
      "        [ 0.0403, -0.0486,  0.0462,  ...,  0.0861,  0.0025,  0.0432],\n",
      "        [-0.1275,  0.0479,  0.1841,  ...,  0.0899, -0.1297, -0.0879],\n",
      "        ...,\n",
      "        [-0.0445, -0.0548,  0.0123,  ...,  0.1044,  0.0978, -0.0695],\n",
      "        [ 0.1860,  0.0167,  0.0461,  ..., -0.0963,  0.0785, -0.0225],\n",
      "        [ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207]],\n",
      "       requires_grad=True)\n",
      "Output shape: torch.Size([1, 35, 768])\n",
      "Reference output shape: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0514, -0.0277,  0.0499,  ...,  0.0070,  0.1552,  0.1207],\n",
       "         [ 0.1474, -0.0959,  0.1430,  ...,  0.1030, -0.0625, -0.1131],\n",
       "         [ 0.1596, -0.1249,  0.1148,  ...,  0.2558,  0.0196,  0.0145],\n",
       "         ...,\n",
       "         [-0.0393,  0.0050,  0.0421,  ..., -0.0477,  0.0670, -0.0471],\n",
       "         [-0.1488,  0.1519,  0.0056,  ..., -0.3107,  0.2073,  0.0377],\n",
       "         [-0.1101, -0.0393,  0.0331,  ..., -0.1364,  0.0151,  0.0453]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        one_hot_tokens = nn.functional.one_hot(tokens, num_classes = cfg.d_vocab).float()\n",
    "        # one_hot_tokens = einops.rearrange(one_hot_tokens, 'b p v -> (b p) v')\n",
    "        print(one_hot_tokens.shape)\n",
    "        print(self.W_E.shape)\n",
    "        print(one_hot_tokens)\n",
    "        print(self.W_E)\n",
    "        return torch.matmul(one_hot_tokens, self.W_E)\n",
    "\n",
    "rand_int_test(Embed, [2, 4])\n",
    "load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fed372-db51-4d61-8fcd-c547ab5b1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
