{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30ddbb8f-4e35-4d04-bb5d-0d82727862fd",
   "metadata": {},
   "source": [
    "[WIP] GPT2 implementation based on [Neel Nanda's Clean Transformer Video Tutorial](https://www.youtube.com/watch?v=bOYE6E8JrtU&list=PL7m7hLIqA0hoIUPhC26ASCVs_VrqcDpAz&index=2&ab_channel=NeelNanda) and Template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29bce165-268a-4c14-a84f-aeaea808a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "import einops\n",
    "import unittest\n",
    "from fancy_einsum import einsum\n",
    "import math\n",
    "from easy_transformer import EasyTransformer\n",
    "from easy_transformer.utils import gelu_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fbcf2c-58d0-4e9f-a671-d559ce64b58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db16fb5f-a994-461b-95de-c56ef57cf3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    max_context: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a342a1-d3e5-47df-8655-d08422c44f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        mean = einops.reduce(residual, 'b p d -> b p', 'mean')\n",
    "        broadcast_mean = einops.repeat(mean,'b p -> b p d', d=cfg.d_model)\n",
    "        residual -= broadcast_mean\n",
    "        std_dev = torch.sqrt(einops.reduce(residual ** 2, 'b p d -> b p', 'mean') + cfg.layer_norm_eps)\n",
    "        broadcast_std_dev = einops.repeat(std_dev, 'b p -> b p d', d=cfg.d_model)\n",
    "        normalized = residual / broadcast_std_dev\n",
    "        return normalized * self.w + self.b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f73161-7e60-41b2-bf7f-233dad4b972a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((cfg.d_vocab, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        one_hot_tokens = nn.functional.one_hot(tokens, num_classes = cfg.d_vocab).float()\n",
    "        return torch.matmul(one_hot_tokens, self.W_E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05c31633-e60b-44c6-a5dd-19b253c07b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((cfg.max_context, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        batch_size, max_tokens = tokens.shape\n",
    "        truncuated_W_pos = self.W_pos[:max_tokens, :]\n",
    "        return torch.broadcast_to(truncuated_W_pos, (batch_size, max_tokens, self.cfg.d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a30a123d-8a54-430b-8c57-ea780e49424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Parameters to calculate queries\n",
    "        self.W_Q = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        # Parameters to calculate keys\n",
    "        self.W_K = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        # Parameters to calculate values\n",
    "        self.W_V = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((cfg.n_heads, cfg.d_head)))\n",
    "\n",
    "        # Parameters to combine head outputs\n",
    "        self.W_O = nn.Parameter(torch.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "        \n",
    "        if IN_COLAB:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\"))\n",
    "        else:\n",
    "            self.register_buffer(\"IGNORE\", torch.tensor(-1e5, dtype=torch.float32, device=\"cpu\"))\n",
    "            \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        \n",
    "        queries = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_Q) + self.b_Q\n",
    "\n",
    "        keys = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_K) + self.b_K\n",
    "\n",
    "        values = einsum(\n",
    "            'batch position d_model, n_heads d_model d_head -> batch position n_heads d_head',\n",
    "            normalized_resid_pre, self.W_V) + self.b_V\n",
    "\n",
    "        prob_dist = self._get_attention(queries, keys)\n",
    "\n",
    "        sum_after_attention = einsum(\n",
    "            'batch key_position n_heads d_head, batch n_heads query_position key_position -> batch n_heads query_position d_head',\n",
    "            values,\n",
    "            prob_dist)\n",
    "        \n",
    "        out = einsum(\n",
    "            'batch n_heads query_position d_head, n_heads d_head d_model -> batch query_position d_model',\n",
    "            sum_after_attention,\n",
    "            self.W_O) + self.b_O\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def _get_attention(self, queries, keys):\n",
    "        attention_scores = einsum(\n",
    "            'batch query_position n_heads d_head, batch key_position n_heads d_head -> batch n_heads query_position key_position',\n",
    "            queries,\n",
    "            keys)\n",
    "        attention_scores = attention_scores / math.sqrt(self.cfg.d_head)\n",
    "        mask = torch.triu(torch.ones(attention_scores.shape[-2], attention_scores.shape[-1]), diagonal=1).bool()\n",
    "        attention_scores.masked_fill_(mask, self.IGNORE)\n",
    "        prob_dist = torch.softmax(attention_scores, dim=3)\n",
    "        return prob_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606efaac-a712-404a-b39a-55d091a10872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(torch.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((cfg.d_model)))\n",
    "    \n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        middle = einsum(\n",
    "            'd_model d_mlp, batch position d_model-> batch position d_mlp',\n",
    "            self.W_in,\n",
    "            normalized_resid_mid) + self.b_in\n",
    "        after_non_lin = gelu_new(middle)\n",
    "        out = einsum(\n",
    "            'd_mlp d_model, batch position d_mlp -> batch position d_model',\n",
    "            self.W_out,\n",
    "            after_non_lin) + self.b_out\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28b93cf7-6cc6-4887-9351-cda927fad60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre [batch, position, d_model]\n",
    "        attn = self.attn(self.ln1(resid_pre))\n",
    "        resid_mid = resid_pre + attn\n",
    "        mlp_out = self.mlp(self.ln2(resid_mid))\n",
    "        out = resid_mid + mlp_out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308164f4-58f4-4273-869d-660f656d8d95",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c268d88f-a757-4c80-896e-67274f17aafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tests(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.reference_gpt2 = cls.get_reference_gpt2()\n",
    "        reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "        cls.tokens = cls.reference_gpt2.to_tokens(reference_text)\n",
    "        if IN_COLAB:\n",
    "            cls.tokens = cls.tokens.cuda()\n",
    "        cls.cache_dict = cls.get_gpt2_cache_dict(cls.tokens)\n",
    "        cls.cfg = Config(debug=True)\n",
    "\n",
    "    @classmethod\n",
    "    def get_reference_gpt2(cls):\n",
    "        if IN_COLAB:\n",
    "            device = 'cuda'\n",
    "        else:\n",
    "            device = 'cpu'\n",
    "        return EasyTransformer.from_pretrained(\n",
    "            \"gpt2-small\",\n",
    "            fold_ln=False,\n",
    "            center_unembed=False,\n",
    "            center_writing_weights=False,\n",
    "            device=device)\n",
    "\n",
    "    @classmethod\n",
    "    def get_gpt2_cache_dict(cls, tokens):    \n",
    "        _, cache = cls.reference_gpt2.run_with_cache(tokens)\n",
    "        return cache.cache_dict\n",
    "\n",
    "    def rand_float_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randn(shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def rand_int_test(self, cls, shape):\n",
    "        layer = cls(self.cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        random_input = torch.randint(100, 1000, shape)\n",
    "        if IN_COLAB:\n",
    "            random_input = random_input.cuda()\n",
    "        output = layer(random_input)\n",
    "        return output\n",
    "\n",
    "    def load_gpt2_test(self, cls, gpt2_layer, input_name):\n",
    "        layer = cls(cfg)\n",
    "        if IN_COLAB:\n",
    "            layer = layer.cuda()\n",
    "        layer.load_state_dict(gpt2_layer.state_dict(), strict=False)\n",
    "        # Allow inputs of strings or tensors\n",
    "        if isinstance(input_name, str):\n",
    "            reference_input = self.cache_dict[input_name]\n",
    "        else:\n",
    "            reference_input = input_name\n",
    "        reference_input = reference_input.cpu()\n",
    "        output = layer(reference_input)\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "        comparison = torch.isclose(output, reference_output, atol=1e-4, rtol=1e-3)\n",
    "        correct_ratio = comparison.sum()/comparison.numel()\n",
    "        self.assertEqual(correct_ratio, 1, f'{torch.round(correct_ratio * 100)}% of values are correct')\n",
    "        return output\n",
    "\n",
    "    def test_layer_norm(self):\n",
    "        self.rand_float_test(LayerNorm, [2, 4, 768])\n",
    "        self.load_gpt2_test(LayerNorm, self.reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")\n",
    "\n",
    "    def test_embed(self):\n",
    "        self.rand_int_test(Embed, [2, 4])\n",
    "        self.load_gpt2_test(Embed, self.reference_gpt2.embed, self.tokens)\n",
    "\n",
    "    def test_pos_embed(self):\n",
    "        self.rand_int_test(PosEmbed, [2, 4])\n",
    "        self.load_gpt2_test(PosEmbed, self.reference_gpt2.pos_embed, self.tokens)\n",
    "\n",
    "    def test_attention(self):\n",
    "        self.rand_float_test(Attention, [2, 4, 768])\n",
    "        self.load_gpt2_test(Attention, self.reference_gpt2.blocks[0].attn, \"blocks.0.ln1.hook_normalized\")\n",
    "\n",
    "    def test_mlp(self):\n",
    "        self.rand_float_test(MLP, [2, 4, 768])\n",
    "        self.load_gpt2_test(MLP, self.reference_gpt2.blocks[0].mlp, \"blocks.0.ln2.hook_normalized\")\n",
    "\n",
    "    def test_transformer_block(self):\n",
    "        self.rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "        self.load_gpt2_test(TransformerBlock, self.reference_gpt2.blocks[0], \"resid_pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fed372-db51-4d61-8fcd-c547ab5b1a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n",
      "Finished loading pretrained model gpt2-small into EasyTransformer!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....E\n",
      "======================================================================\n",
      "ERROR: test_transformer_block (__main__.Tests)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_9054/882097626.py\", line 91, in test_transformer_block\n",
      "    self.load_gpt2_test(TransformerBlock, self.reference_gpt2.blocks[0], \"resid_pre\")\n",
      "  File \"/tmp/ipykernel_9054/882097626.py\", line 58, in load_gpt2_test\n",
      "    reference_input = self.cache_dict[input_name]\n",
      "KeyError: 'resid_pre'\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 5.219s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=6 errors=1 failures=0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suite = unittest.TestSuite()\n",
    "suite.addTest(Tests('test_layer_norm'))\n",
    "suite.addTest(Tests('test_embed'))\n",
    "suite.addTest(Tests('test_pos_embed'))\n",
    "suite.addTest(Tests('test_attention'))\n",
    "suite.addTest(Tests('test_mlp'))\n",
    "suite.addTest(Tests('test_transformer_block'))\n",
    "\n",
    "runner = unittest.TextTestRunner()\n",
    "runner.run(suite)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
